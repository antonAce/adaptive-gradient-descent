{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "gradient_descent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIttKz-mrKtQ"
      },
      "source": [
        "# Adaptive Stochastic Gradient Descent\n",
        "\n",
        "Implementation and comparison of adaptive stochastic gradient descent methods using Python and TensorFlow framework. The efficiency comparison is demonstrated in the neural language processing problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2BS-jAgrKtU"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The gradient descent method is a key algorithm in backpropagation optimization for neural networks. Aside hyperparameter tuning, there are various ways to improve the efficiency and speed of learning like, for instance, applying **momentum** to the gradient or by using batching.\n",
        "\n",
        "In this overview, the following gradient algorithms will be implemented using TensorFlow API to speed up the process of gradient calculation in the neural language processing problem using [Calculation Graph](https://www.tensorflow.org/api_docs/python/tf/Graph)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cvn-uUebrKtX"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hxi4Z5tF7C0c"
      },
      "source": [
        "## Tensorflow API usage\n",
        "\n",
        "The gradient value for the following methods will be calculated using [tf.GradientTape()](https://www.tensorflow.org/api_docs/python/tf/GradientTape) in order to achieve higher performance and calculation parallelism. The following code demonstrates class usage in the optimization problem for the **Rosenbrock function**:\n",
        "\n",
        "$F(x, y) = (1 - x)^{2} + 100(y - x^{2})^{2}$\n",
        "\n",
        "Function has a single local minimum in $(1, 1)$ and is equal $0$ in the following point.\n",
        "\n",
        "The gradient value, by definition, is a vector that points towards the direction of the greatest increase of the function. In order to reach a local minimum using an iterative algorithm, the next step should be in the direction opposite to the gradient value. So the full algorithm of finding local minimum can be described as $x_{i+1} = x_{i} - \\lambda_{i}\\nabla F$, where $\\lambda_{i} = const$ - size of a single step, $x_{i}$ - current step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vUIAbHGDosX",
        "outputId": "dd6e9413-5c5b-4a9b-80b3-91d286d08466",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Single iteration of an algorithm\n",
        "def gradient_step(alpha: tf.constant, x: tf.Variable, y: tf.Variable) -> tf.Variable:\n",
        "  with tf.GradientTape() as grad:\n",
        "    grad.watch((alpha, x, y))\n",
        "\n",
        "    # Optimization function\n",
        "    F = (1.0 - x) ** 2.0 + 100.0 * (y - x ** 2.0) ** 2.0\n",
        "    [dF_dx, dF_dy] = grad.gradient(F, [x, y])\n",
        "\n",
        "  return x - alpha * tf.Variable(dF_dx), y - alpha * tf.Variable(dF_dy)\n",
        "\n",
        "# Gradient steps\n",
        "steps = 1000\n",
        "\n",
        "# Gradient step size\n",
        "alpha = tf.constant(0.001)\n",
        "\n",
        "# Start conditions\n",
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(-1.0)\n",
        "\n",
        "# Iterations\n",
        "xy = []\n",
        "\n",
        "for _ in range(steps):\n",
        "  # Iterative descent\n",
        "  x, y = gradient_step(alpha, x, y)\n",
        "\n",
        "  xy.append((x.numpy(), y.numpy()))\n",
        "\n",
        "print(f\"Closest approximation of the local minimum: {xy[-1]}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Closest approximation of the local minimum: (0.9669684, 0.9348931)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5qvxjPtrKtl"
      },
      "source": [
        "## Attribution\n",
        "\n",
        "Overview is based on research paper \"An overview of gradient descent optimization algorithms\" from [arXiv.org](https://arxiv.org/pdf/1609.04747.pdf) by [Sebastian Ruder](mailto:ruder.sebastian@gmail.com) licensed under CC BY-NC-SA 4.0."
      ]
    }
  ]
}