{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "gradient_descent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIttKz-mrKtQ"
      },
      "source": [
        "# Adaptive Stochastic Gradient Descent\n",
        "\n",
        "Implementation and comparison of adaptive stochastic gradient descent methods using Python and TensorFlow framework. The efficiency comparison is demonstrated in the neural language processing problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2BS-jAgrKtU"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The gradient descent method is a key algorithm in backpropagation optimization for neural networks. Aside hyperparameter tuning, there are various ways to improve the efficiency and speed of learning like, for instance, applying **momentum** to the gradient or by using batching.\n",
        "\n",
        "In this overview, the following gradient algorithms will be implemented using TensorFlow API to speed up the process of gradient calculation in the neural language processing problem using [Calculation Graph](https://www.tensorflow.org/api_docs/python/tf/Graph)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cvn-uUebrKtX"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hxi4Z5tF7C0c"
      },
      "source": [
        "## Tensorflow API usage\n",
        "\n",
        "The gradient value for the following methods will be calculated using [tf.GradientTape()](https://www.tensorflow.org/api_docs/python/tf/GradientTape) in order to achieve higher performance and calculation parallelism. The following code demonstrates class usage in the optimization problem for the **Rosenbrock function**:\n",
        "\n",
        "$F(x, y) = (1 - x)^{2} + 100(y - x^{2})^{2}$\n",
        "\n",
        "Function has a single local minimum in $(1, 1)$ and is equal $0$ in the following point. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vUIAbHGDosX"
      },
      "source": [
        "# Gradient step\n",
        "steps = 1000\n",
        "alpha = tf.constant(0.0)\n",
        "\n",
        "# Start conditions\n",
        "x = tf.Variable([0.0, 3.0])\n",
        "\n",
        "with tf.GradientTape() as grad:\n",
        "\n",
        "  # Optimization function\n",
        "  F = (1.0 - x[0]) ** 2.0 + 100.0 * (x[1] - x[0] ** 2.0) ** 2.0\n",
        "\n",
        "  for _ in range(steps):\n",
        "    x -= alpha * grad.gradient(F, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5qvxjPtrKtl"
      },
      "source": [
        "## Attribution\n",
        "\n",
        "Overview is based on research paper \"An overview of gradient descent optimization algorithms\" from [arXiv.org](https://arxiv.org/pdf/1609.04747.pdf) by [Sebastian Ruder](mailto:ruder.sebastian@gmail.com) licensed under CC BY-NC-SA 4.0."
      ]
    }
  ]
}