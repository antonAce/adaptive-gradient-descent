# Adaptive Stochastic Gradient Descent

**The goal of the research:** Implementation and comparison of adaptive stochastic gradient descent methods using Python and TensorFlow framework. The efficiency comparison is demonstrated in the logistic regression optimization problem.

### Attribution

Overview is based on research paper "An overview of gradient descent optimization algorithms" from [arXiv.org](https://arxiv.org/pdf/1609.04747.pdf) by [Sebastian Ruder](mailto:ruder.sebastian@gmail.com) licensed under CC BY-NC-SA 4.0.

### Overviewed gradient algorithms

- SGD
- Nesterov AG
- ADAGRAD
- RMSPROP
- ADAM
- Dynamic Sampling SGD
- Mini-batch SGD
