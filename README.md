# Adaptive Stochastic Gradient Descent

**The goal of the research:** Implementation and comparison of adaptive stochastic gradient descent methods using Python and TensorFlow framework. The efficiency comparison is demonstrated in the neural language processing problem.

### Overviewed gradient algorithms

- SGD
- Nesterov AG
- ADAGRAD
- RMSPROP
- ADAM
- Dynamic Sampling SGD
- Mini-batch SGD
